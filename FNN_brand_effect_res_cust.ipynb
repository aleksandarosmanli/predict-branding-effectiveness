{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e0e1271-053c-4de2-994a-142de1b8787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69959203-a1ea-4119-82c8-1a02ff650390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inductively_brand_aware</th>\n",
       "      <th>spontaneously_brand_aware</th>\n",
       "      <th>brand_distinct</th>\n",
       "      <th>brand_quality</th>\n",
       "      <th>intend_to_buy</th>\n",
       "      <th>reason_try</th>\n",
       "      <th>reason_competition</th>\n",
       "      <th>reason_repeat</th>\n",
       "      <th>reason_premium</th>\n",
       "      <th>reason_superiority</th>\n",
       "      <th>after_purchase_satisfiedness</th>\n",
       "      <th>recommend</th>\n",
       "      <th>brandful</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   inductively_brand_aware  spontaneously_brand_aware  brand_distinct  \\\n",
       "0                      0.0                        1.0             1.0   \n",
       "1                      1.0                        0.0             1.0   \n",
       "2                      0.5                        0.0             1.0   \n",
       "3                      0.0                        0.5             1.0   \n",
       "4                      0.0                        1.0             0.0   \n",
       "\n",
       "   brand_quality  intend_to_buy  reason_try  reason_competition  \\\n",
       "0            0.0            0.0         0.0                 0.5   \n",
       "1            0.0            1.0         0.0                 0.0   \n",
       "2            0.0            1.0         0.0                 1.0   \n",
       "3            1.0            1.0         0.0                 1.0   \n",
       "4            1.0            0.0         0.0                 0.0   \n",
       "\n",
       "   reason_repeat  reason_premium  reason_superiority  \\\n",
       "0            1.0             0.0                 0.0   \n",
       "1            0.0             0.0                 0.0   \n",
       "2            1.0             0.0                 0.0   \n",
       "3            1.0             0.0                 0.0   \n",
       "4            0.0             0.0                 0.0   \n",
       "\n",
       "   after_purchase_satisfiedness  recommend  brandful  target  \n",
       "0                           1.0        1.0       1.0       2  \n",
       "1                           1.0        0.5       0.0       1  \n",
       "2                           1.0        1.0       1.0       2  \n",
       "3                           0.0        1.0       1.0       2  \n",
       "4                           0.0        1.0       1.0       1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "filepath = os.path.join('..', 'datasets', 'brand_effect_res_cust.csv')\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "y = df['target']\n",
    "\n",
    "# Drop `ID` column\n",
    "df = df.drop(columns=['customer_id'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a456f955-fd4e-422b-913c-9c34a68755e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "2    4086\n",
       "3    1662\n",
       "1    1188\n",
       "4      32\n",
       "0      32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class balance of 'target' col\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "befec268-9091-42fd-991a-38e168f6df5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "2    4086\n",
       "3    1662\n",
       "1    1188\n",
       "4     332\n",
       "0     332\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define number of synthetic samples per class\n",
    "num_samples_per_class = 300\n",
    "\n",
    "# Define weight dictionary for the 13 features\n",
    "feature_weights = {\n",
    "    \"inductively_brand_aware\": 0.80, \"spontaneously_brand_aware\": 0.85, \"brand_distinct\": 0.78, \"brand_quality\": 0.82,\n",
    "    \"intend_to_buy\": 0.88, \"reason_try\": 0.65, \"reason_competition\": 0.70, \"reason_repeat\": 0.90,\n",
    "    \"reason_premium\": 0.45, \"reason_superiority\": 0.75, \"after_purchase_satisfiedness\": 0.92, \"recommend\": 0.85, \"brandful\": 0.87\n",
    "}\n",
    "\n",
    "# Function to compute normalized score\n",
    "def compute_normalized_score(row):\n",
    "    row_values = np.array([row[feature] for feature in feature_weights.keys()])\n",
    "    weight_values = np.array(list(feature_weights.values()))\n",
    "    return np.dot(row_values, weight_values) / np.sum(weight_values)\n",
    "\n",
    "\n",
    "# Function to generate synthetic data ensuring exactly 150 samples per class\n",
    "def generate_synthetic_data(num_samples, low_values, high_values, condition, target_label):\n",
    "    synthetic_samples = []\n",
    "    while len(synthetic_samples) < num_samples:\n",
    "        sample = np.random.choice(low_values if target_label == 0 else high_values, size=(13,))\n",
    "        normalized_score = np.dot(sample, list(feature_weights.values())) / np.sum(list(feature_weights.values()))\n",
    "        if condition(normalized_score):\n",
    "            synthetic_samples.append(list(sample) + [normalized_score, target_label])\n",
    "    \n",
    "    # Convert the synthetic_samples list into a pandas DataFrame\n",
    "    column_names = list(feature_weights.keys()) + ['normalized_score', 'target']\n",
    "    return pd.DataFrame(synthetic_samples, columns=column_names)\n",
    "\n",
    "\n",
    "# Generate synthetic data for class 0 (low scores, choose from 0 and 0.5)\n",
    "synthetic_class_0 = generate_synthetic_data(num_samples_per_class, [0, 0.5], [0.5, 1], lambda x: x < 0.2, 0)\n",
    "synthetic_class_0[\"target\"] = 0\n",
    "\n",
    "# Generate synthetic data for class 4 (high scores, choose from 0.5 and 1)\n",
    "synthetic_class_4 = generate_synthetic_data(num_samples_per_class, [0, 0.5], [0.5, 1], lambda x: x >= 0.8, 4)\n",
    "synthetic_class_4[\"target\"] = 4\n",
    "\n",
    "\n",
    "# Merge synthetic samples into dataset\n",
    "df = pd.concat([df, synthetic_class_0, synthetic_class_4], ignore_index=True)\n",
    "\n",
    "# Drop temporary score column\n",
    "df.drop(columns=[\"normalized_score\"], inplace=True)\n",
    "\n",
    "# Get class balance of 'target' col\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d655407-65ec-4ad6-8d62-56a9bcb0cbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution: (array([0, 1, 2, 3, 4], dtype=int64), array([ 234,  838, 2882, 1173,  234], dtype=int64))\n",
      "Validation class distribution: (array([0, 1, 2, 3, 4], dtype=int64), array([ 48, 172, 591, 240,  48], dtype=int64))\n",
      "Test class distribution: (array([0, 1, 2, 3, 4], dtype=int64), array([ 50, 178, 613, 249,  50], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# 1. Isolate X variables\n",
    "X = df.drop(columns=['target'])\n",
    "\n",
    "# 2. Isolate y variable\n",
    "y = df['target']\n",
    "\n",
    "# 3. Split into train and test sets\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X, y, stratify=y,\n",
    "                                              test_size=0.15, random_state=42)\n",
    "\n",
    "# 4. Split into train and validate sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, stratify=y_tr,\n",
    "                                                  test_size=0.17, random_state=42)\n",
    "\n",
    "# Print class distributions\n",
    "print(\"Train class distribution:\", np.unique(y_train, return_counts=True))\n",
    "print(\"Validation class distribution:\", np.unique(y_val, return_counts=True))\n",
    "print(\"Test class distribution:\", np.unique(y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f05f60f-7278-4676-b47f-ec3b1680829f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5361\n",
      "1099\n",
      "1140\n"
     ]
    }
   ],
   "source": [
    "for x in [X_train, X_val, X_test]:\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f71d3ff-b72b-4826-84d8-b3bc90a55f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (5361, 13)\n",
      "Shape of y_train: (5361,)\n",
      "Shape of X_val: (1099, 13)\n",
      "Shape of y_val: (1099,)\n",
      "Shape of X_test: (1140, 13)\n",
      "Shape of y_test: (1140,)\n"
     ]
    }
   ],
   "source": [
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_train = X_train_scaled\n",
    "\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_val = X_val_scaled\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test = X_test_scaled\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_val:\", X_val.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c71abfa9-7941-4d6b-807e-3df699ba6d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 13      # number of input features\n",
    "n_y = 5       # number of ouputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "276eafe2-0160-4288-9b2d-345de1db794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(n_h1, n_h2):\n",
    "\n",
    "    # Define the input layer separately\n",
    "    input_layer = layers.Input(shape=(n_x,))  # Define the input layer explicitly\n",
    "\n",
    "    # Define the rest of the model\n",
    "    x = layers.Dense(n_h1, activation='relu')(input_layer)  # First hidden layer\n",
    "    x = layers.Dense(n_h2, activation='relu')(x)  # Second hidden layer\n",
    "    output_layer = layers.Dense(n_y, activation='softmax')(x)  # Output layer (binary classification)\n",
    "\n",
    "    # Create the model by specifying inputs and outputs\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=150, \n",
    "        batch_size=16, \n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Training set\n",
    "    \n",
    "    # Get predictions\n",
    "    y_train_pred_probs = model.predict(X_train)  # Probabilities\n",
    "    y_train_pred = y_train_pred_probs.argmax(axis=1)  # Convert to class predictions\n",
    "    \n",
    "    # Compute metrics\n",
    "    f1_macro_train = f1_score(y_train, y_train_pred, average=\"macro\")\n",
    "    f1_weighted_train = f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "    recall_macro_train = recall_score(y_train, y_train_pred, average=\"macro\")\n",
    "    roc_auc_ovr_train = roc_auc_score(y_train, y_train_pred_probs, average=\"macro\", multi_class=\"ovr\")\n",
    "    \n",
    "    print(f\"Training Metrics:\")\n",
    "    print(f\"F1 Macro: {f1_macro_train:.4f}\")\n",
    "    print(f\"F1 Weighted: {f1_weighted_train:.4f}\")\n",
    "    print(f\"Recall Macro: {recall_macro_train:.4f}\")\n",
    "    print(f\"ROC AUC OVR: {roc_auc_ovr_train:.4f}\")\n",
    "    \n",
    "    \n",
    "    # Validation set\n",
    "    \n",
    "    # Get predictions\n",
    "    y_val_pred_probs = model.predict(X_val)  # Probabilities\n",
    "    y_val_pred = y_val_pred_probs.argmax(axis=1)  # Convert to class predictions\n",
    "    \n",
    "    # Compute metrics\n",
    "    f1_macro_val = f1_score(y_val, y_val_pred, average=\"macro\")\n",
    "    f1_weighted_val = f1_score(y_val, y_val_pred, average=\"weighted\")\n",
    "    recall_macro_val = recall_score(y_val, y_val_pred, average=\"macro\")\n",
    "    roc_auc_ovr_val = roc_auc_score(y_val, y_val_pred_probs, average=\"macro\", multi_class=\"ovr\")\n",
    "    \n",
    "    print(f\"Validation Metrics:\")\n",
    "    print(f\"F1 Macro: {f1_macro_val:.4f}\")\n",
    "    print(f\"F1 Weighted: {f1_weighted_val:.4f}\")\n",
    "    print(f\"Recall Macro: {recall_macro_val:.4f}\")\n",
    "    print(f\"ROC AUC OVR: {roc_auc_ovr_val:.4f}\")\n",
    "    \n",
    "    \n",
    "    # Test set\n",
    "    \n",
    "    # Get predictions\n",
    "    y_test_pred_probs = model.predict(X_test)  # Probabilities\n",
    "    y_test_pred = y_test_pred_probs.argmax(axis=1)  # Convert to class predictions\n",
    "    \n",
    "    # Compute metrics\n",
    "    f1_macro_test = f1_score(y_test, y_test_pred, average=\"macro\")\n",
    "    f1_weighted_test = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "    recall_macro_test = recall_score(y_test, y_test_pred, average=\"macro\")\n",
    "    roc_auc_ovr_test = roc_auc_score(y_test, y_test_pred_probs, average=\"macro\", multi_class=\"ovr\")\n",
    "    \n",
    "    print(f\"Test Metrics:\")\n",
    "    print(f\"F1 Macro: {f1_macro_test:.4f}\")\n",
    "    print(f\"F1 Weighted: {f1_weighted_test:.4f}\")\n",
    "    print(f\"Recall Macro: {recall_macro_test:.4f}\")\n",
    "    print(f\"ROC AUC OVR: {roc_auc_ovr_test:.4f}\")\n",
    "\n",
    "    return model, f1_macro_test, f1_weighted_test, recall_macro_test, roc_auc_ovr_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f78ebf8-bd67-460c-8829-074d9655cb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FNN with H1=26, H2=13\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Training Metrics:\n",
      "F1 Macro: 0.9908\n",
      "F1 Weighted: 0.9927\n",
      "Recall Macro: 0.9914\n",
      "ROC AUC OVR: 1.0000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Validation Metrics:\n",
      "F1 Macro: 0.9817\n",
      "F1 Weighted: 0.9835\n",
      "Recall Macro: 0.9820\n",
      "ROC AUC OVR: 0.9997\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Test Metrics:\n",
      "F1 Macro: 0.9851\n",
      "F1 Weighted: 0.9851\n",
      "Recall Macro: 0.9871\n",
      "ROC AUC OVR: 0.9997\n",
      "Training FNN with H1=39, H2=19\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Training Metrics:\n",
      "F1 Macro: 0.9973\n",
      "F1 Weighted: 0.9989\n",
      "Recall Macro: 0.9955\n",
      "ROC AUC OVR: 1.0000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Validation Metrics:\n",
      "F1 Macro: 0.9935\n",
      "F1 Weighted: 0.9909\n",
      "Recall Macro: 0.9927\n",
      "ROC AUC OVR: 0.9997\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Test Metrics:\n",
      "F1 Macro: 0.9851\n",
      "F1 Weighted: 0.9894\n",
      "Recall Macro: 0.9788\n",
      "ROC AUC OVR: 0.9999\n",
      "Training FNN with H1=52, H2=26\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Training Metrics:\n",
      "F1 Macro: 1.0000\n",
      "F1 Weighted: 1.0000\n",
      "Recall Macro: 1.0000\n",
      "ROC AUC OVR: 1.0000\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Validation Metrics:\n",
      "F1 Macro: 0.9883\n",
      "F1 Weighted: 0.9863\n",
      "Recall Macro: 0.9887\n",
      "ROC AUC OVR: 0.9998\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Test Metrics:\n",
      "F1 Macro: 0.9851\n",
      "F1 Weighted: 0.9895\n",
      "Recall Macro: 0.9867\n",
      "ROC AUC OVR: 0.9999\n",
      "\n",
      "Best test results come from FNN (13,52,26,5)\n",
      "F1 Macro: 0.9851\n",
      "F1 Weighted: 0.9895\n",
      "Recall Macro: 0.9867\n",
      "ROC AUC OVR: 0.9999\n",
      "   f1_macro  f1_weighted  recall_macro  roc_auc_ovr\n",
      "0  0.985143     0.989502      0.986666     0.999913\n"
     ]
    }
   ],
   "source": [
    "n_h1=0\n",
    "n_h2=0\n",
    "best_results = {\n",
    "    'f1_macro' : 0,\n",
    "    'f1_weighted' : 0,\n",
    "    'recall_macro' : 0,\n",
    "    'roc_auc_ovr' : 0\n",
    "}\n",
    "sum_best_results = 0\n",
    "h1_candidates = [2 * n_x, 3 * n_x, 4 * n_x]\n",
    "h2_candidates = [h1 // 2 for h1 in h1_candidates]\n",
    "\n",
    "for h1, h2 in zip(h1_candidates, h2_candidates):\n",
    "    print(f\"Training FNN with H1={h1}, H2={h2}\")\n",
    "    \n",
    "    model, f1_macro, f1_weighted, recall_macro, roc_auc_ovr = predict(h1, h2)\n",
    "    \n",
    "    sum_results = f1_macro + f1_weighted + recall_macro + roc_auc_ovr\n",
    "    \n",
    "    if sum_results > sum_best_results:\n",
    "        sum_best_results = sum_results\n",
    "        best_results['f1_macro'] = f1_macro\n",
    "        best_results['f1_weighted'] = f1_weighted\n",
    "        best_results['recall_macro'] = recall_macro\n",
    "        best_results['roc_auc_ovr'] = roc_auc_ovr\n",
    "        n_h1 = h1\n",
    "        n_h2 = h2\n",
    "            \n",
    "print(\"\")\n",
    "print(f'Best test results come from FNN ({n_x},{n_h1},{n_h2},{n_y})')\n",
    "\n",
    "print(f'F1 Macro: {best_results[\"f1_macro\"]:.4f}')\n",
    "print(f'F1 Weighted: {best_results[\"f1_weighted\"]:.4f}')\n",
    "print(f'Recall Macro: {best_results[\"recall_macro\"]:.4f}')\n",
    "print(f'ROC AUC OVR: {best_results[\"roc_auc_ovr\"]:.4f}')\n",
    "\n",
    "results_df = pd.DataFrame([best_results])  # Use [best_results] to make it a single-row DataFrame\n",
    "\n",
    "# Step 3: Export to Excel\n",
    "results_df.to_excel('be_res_best_FNN_results.xlsx', index=False)\n",
    "\n",
    "# Optional: Display DataFrame\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45e409ad-94b4-47f4-b7db-5a4aa76b6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_to_probability_delta(shap_val):\n",
    "    return np.tanh(shap_val)  # maps raw SHAP value to ~[-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e318cc2-edff-4d7e-adb2-30acb5d28855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Imp. %</th>\n",
       "      <th>Imp. Direct.</th>\n",
       "      <th>Imp. Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reason_try</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Inc.</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reason_competition</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Inc.</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reason_premium</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Inc.</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reason_superiority</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Inc.</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brandful</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Dec.</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>inductively_brand_aware</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Dec.</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brand_quality</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Dec.</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>after_purchase_satisfiedness</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Dec.</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>recommend</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Dec.</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spontaneously_brand_aware</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Dec.</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>intend_to_buy</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Dec.</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>brand_distinct</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Dec.</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>reason_repeat</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Dec.</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Feature  Imp. % Imp. Direct. Imp. Strength\n",
       "0                     reason_try     1.1         Inc.           Low\n",
       "1             reason_competition     1.1         Inc.           Low\n",
       "2                 reason_premium     1.0         Inc.           Low\n",
       "3             reason_superiority     0.5         Inc.           Low\n",
       "4                       brandful     2.4         Dec.          High\n",
       "5        inductively_brand_aware     1.9         Dec.          High\n",
       "6                  brand_quality     1.9         Dec.          High\n",
       "7   after_purchase_satisfiedness     1.9         Dec.          High\n",
       "8                      recommend     1.9         Dec.        Medium\n",
       "9      spontaneously_brand_aware     1.8         Dec.        Medium\n",
       "10                 intend_to_buy     1.8         Dec.        Medium\n",
       "11                brand_distinct     1.3         Dec.        Medium\n",
       "12                 reason_repeat     0.1         Dec.           Low"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample background data from training set (DeepExplainer needs this for reference)\n",
    "background = X_train[:100]\n",
    "\n",
    "# Initialize SHAP DeepExplainer\n",
    "explainer = shap.DeepExplainer(model, background)\n",
    "\n",
    "# Compute SHAP values for the dataset you want to explain (usually X_train or X_test)\n",
    "shap_values = explainer.shap_values(X_train)  # Ensure data is in NumPy format\n",
    "\n",
    "# Extract SHAP values for class 1 (positive class) from shap_values\n",
    "shap_values_class_1 = shap_values[:, :, 0]\n",
    "\n",
    "# Compute the mean SHAP value per feature (across all samples)\n",
    "mean_shap_values = np.mean(shap_values_class_1, axis=0)  # Mean SHAP value per feature for positive class\n",
    "\n",
    "# Compute mean absolute SHAP values per feature\n",
    "mean_abs_shap_values = np.mean(np.abs(shap_values_class_1), axis=0)  # Mean absolute SHAP values per feature\n",
    "\n",
    "# Direction: 'Increases' if the mean SHAP value for a feature is positive, 'Decreases' otherwise\n",
    "direction = ['Inc.' if val > 0 else 'Dec.' for val in mean_shap_values]\n",
    "\n",
    "# Binning the impact into categories (Low, Medium, High) based on quantiles of the mean absolute SHAP values\n",
    "impact_strength = pd.qcut(mean_abs_shap_values, q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "\n",
    "# Create a DataFrame to summarize the SHAP values for each feature\n",
    "shap_summary_df = pd.DataFrame({\n",
    "'Feature': X.columns,  # Features in the dataset\n",
    "'Imp. %': np.round(shap_to_probability_delta(mean_abs_shap_values)*100, 1),  # Impact percentage for each feature\n",
    "'Imp. Direct.': direction,  # Impact direction based on mean SHAP values\n",
    "'Imp. Strength': impact_strength  # Categorized impact strength (Low, Medium, High)\n",
    "}).sort_values(by=['Imp. Direct.', 'Imp. %'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save the SHAP summary to Excel for further inspection\n",
    "shap_summary_df.to_excel('be_res_shap_summary_fnn.xlsx', index=False)\n",
    "\n",
    "# Display the summary DataFrame\n",
    "shap_summary_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
